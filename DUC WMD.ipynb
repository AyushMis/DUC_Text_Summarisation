{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from Utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "import nbimporter\n",
    "import warnings\n",
    "import gensim\n",
    "import nltk\n",
    "import math\n",
    "import sys\n",
    "import imp\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import Utilities as utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('../dataset/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_pair = utility.loadObj('test_doc_summary_pair')\n",
    "combined = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 181/298 [09:30<12:58,  6.66s/it] "
     ]
    }
   ],
   "source": [
    "for pair in tqdm(doc_summary_pair):\n",
    "    doc = pair[0]\n",
    "    sentence_list = doc.split('.')\n",
    "    included_sentences = []\n",
    "    clean_sentence_list = []\n",
    "    for sentence in sentence_list:\n",
    "        if (len(utility.simpleRemoveStopWords(sentence)) > 3):\n",
    "            clean_sentence_list.append(utility.simpleRemoveStopWords(sentence))\n",
    "            included_sentences.append(sentence)\n",
    "\n",
    "    sentence_distance = np.zeros((len(clean_sentence_list), len(clean_sentence_list)), dtype = float)\n",
    "    for i in range(len(clean_sentence_list)):\n",
    "        for j in range(len(clean_sentence_list)):\n",
    "            dist = model.wmdistance(clean_sentence_list[i], clean_sentence_list[j])\n",
    "            sentence_distance[i][j] = dist\n",
    "            sentence_distance[j][i] = dist\n",
    "\n",
    "    average_sentence_distance = 0\n",
    "    original_sequence = {}\n",
    "    for i in range(0,len(sentence_distance)):\n",
    "        average_sentence_distance = sum(sentence_distance[i])/len(sentence_distance)\n",
    "        original_sequence[i] = average_sentence_distance\n",
    "\n",
    "    ordered_dic = collections.OrderedDict(sorted(original_sequence.items(), key=lambda x: x[1]))\n",
    "\n",
    "    compression_ratio = 40\n",
    "    num_of_sentences = (compression_ratio*len(original_sequence))//100\n",
    "\n",
    "    summary = []\n",
    "    i = 1\n",
    "    for index in ordered_dic:\n",
    "        summary.append(included_sentences[index])\n",
    "        if i == num_of_sentences:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    final_summary = ' '.join(summary)\n",
    "    human_summary = pair[1]\n",
    "\n",
    "    human_summary = ' '.join(utility.simpleRemoveStopWords(human_summary))\n",
    "    summaryBigram = utility.findGrams(final_summary.lower(), 2)\n",
    "    humanBigram = utility.findGrams(human_summary.lower(), 2)\n",
    "    count = 0\n",
    "    for element in humanBigram:\n",
    "        if element in summaryBigram:\n",
    "            count+=1\n",
    "            \n",
    "\n",
    "    result = count/len(humanBigram)\n",
    "        \n",
    "    combined.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.225]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(combined)/len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.sort(reverse = True)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
